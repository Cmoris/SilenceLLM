# SilenceLLM
# Multimodal LLM Training (Audio + Video + Text)

> Training script for a **multimodal large language model** that jointly learns from **audio**, **video**, and **text** data.  
> Built with [PyTorch](https://pytorch.org/), [Transformers](https://huggingface.co/docs/transformers), and [torchrun](https://pytorch.org/docs/stable/elastic/run.html).

---

## ðŸ§© Overview

This repository provides a training pipeline for **SilenceQwen3**, a multimodal extension of the [Qwen3-1.7B](https://huggingface.co/Qwen/Qwen3-1.7B) model.  
It integrates:
- **Audio encoder**: `openai/whisper-medium`
- **Vision encoder**: `google/siglip2-base-patch16-224`
- **Language backbone**: Qwen3-1.7B
- **Q-Former** for modality alignment (`bert-large-uncased`)
- **LoRA fine-tuning** for efficient parameter adaptation

---

## Installation

```bash
pip install torch torchvision torchaudio
pip install transformers accelerate peft
pip install pillow tqdm
```

---

## Project

```bash

project_root/
â”œâ”€â”€ train.py
â”œâ”€â”€ eval.py
â”œâ”€â”€ silence_trainer.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€collector.py
â”‚   â”œâ”€â”€dataset.py
â”‚   â””â”€â”€mm_utils.py
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ submodels
â”‚   â”œâ”€â”€ silence_llama.py
â”‚   â”œâ”€â”€ silence_model.py
â”‚   â”œâ”€â”€ silence_perceiver.py
â”‚   â””â”€â”€ silence_qwen.py  
â”œâ”€â”€ script/
â”‚   â”œâ”€â”€train.sh
â”‚   â””â”€â”€eval.sh

